# üîã Predi√ß√£o do Estado de Sa√∫de (SOH) de Baterias de √çons de L√≠tio (Li-ion) de EVs

## üìà Regress√£o Linear Implementada em Assembly MIPS

Este projeto apresenta uma implementa√ß√£o de um algoritmo de **Regress√£o Linear Simples** desenvolvida em **Assembly MIPS**. O objetivo principal √© prever o **Estado de Sa√∫de (State of Health - SOH)** de baterias de √≠ons de l√≠tio, comumente utilizadas em Ve√≠culos El√©tricos (EVs), utilizando dados de degrada√ß√£o da bateria.

A escolha do MIPS visa explorar a execu√ß√£o de c√°lculos matem√°ticos e manipula√ß√£o de dados em um ambiente de baixo n√≠vel, otimizando o desempenho e demonstrando a viabilidade de modelos de Machine Learning (ML) em arquiteturas de processador mais simples ou embarcadas.

---

## üõ†Ô∏è Tecnologias e Arquivos do Projeto

### Linguagens e Plataformas

* **Assembly MIPS:** Linguagem de montagem utilizada para a implementa√ß√£o do algoritmo principal.
* **MARS (MIPS Assembly and Runtime Simulator):** Simulador MIPS necess√°rio para compilar e executar o programa `Algoritmo_final.mars`.
* **Python:** Utilizado na implementa√ß√£o de refer√™ncia (`regressaoLinear.py`) e para a prepara√ß√£o dos dados.

### Estrutura de Arquivos

| Arquivo | Descri√ß√£o |
| :--- | :--- |
| `Algoritmo_final.mars` | **Programa Principal em MIPS.** Cont√©m a l√≥gica de Regress√£o Linear para carregar os dados, treinar o modelo e realizar predi√ß√µes de SOH. |
| `batteryDataset.csv` | **Banco de Dados Ordenado.** Conjunto de dados original (ou vers√£o limpa) contendo as vari√°veis (e.g., Ciclos de Carga, SOH) para treinamento e teste. |
| `datasetEmbaralhado` | **Banco de Dados Embaralhado.** Vers√£o do dataset com as linhas aleatorizadas. Essencial para garantir que os conjuntos de treino e teste sejam homog√™neos e representativos da distribui√ß√£o total dos dados. |
| `regressaoLinear.py` | **Implementa√ß√£o de Refer√™ncia em Python.** Vers√£o inicial do algoritmo de Regress√£o Linear. Serve como base de valida√ß√£o e como guia l√≥gico para a tradu√ß√£o para Assembly MIPS. |

---

## üöÄ Como Executar o Projeto

Para executar a implementa√ß√£o em MIPS, √© necess√°rio utilizar o simulador **MARS**.

### 1. Pr√©-requisitos

* **Simulador MARS:** Certifique-se de ter o simulador MARS (MIPS Assembly and Runtime Simulator) instalado.

### 2. Configura√ß√£o

1.  **Carregue o Programa:** Abra o simulador MARS.
2.  **Abrir Arquivo:** Carregue o arquivo `Algoritmo_final.mars` no simulador.
3.  **Configurar Dados:** O programa MIPS √© configurado para ler dados de entrada espec√≠ficos. **Certifique-se de que o arquivo de dados a ser lido (geralmente o `datasetEmbaralhado` ou uma vers√£o pr√©-processada compat√≠vel com o formato de leitura do MIPS) esteja na mesma pasta do `Assembly MIPS` ou que o caminho do arquivo no c√≥digo MIPS esteja correto.**
4.  **Monte:** Clique no bot√£o **"Assemble"** (ou use F3).

### 3. Execu√ß√£o

1.  **Execute:** Clique no bot√£o **"Run"** (ou use F5).
2.  **Acompanhamento:** A execu√ß√£o no console (aba *Run I/O*) exibir√° a itera√ß√£o do algoritmo, os valores calculados para os par√¢metros do modelo ($\theta_0$ e $\theta_1$), o erro (fun√ß√£o de custo) e, idealmente, os resultados das predi√ß√µes de SOH.

> **Nota:** A complexidade da manipula√ß√£o de ponto flutuante e a leitura de arquivos em MIPS exigem que os dados de entrada sejam formatados de maneira espec√≠fica (e.g., armazenados em mem√≥ria como n√∫meros de ponto flutuante IEEE 754, ou como inteiros que representam os valores). Consulte o cabe√ßalho de `Algoritmo_final.mars` para entender o formato exato esperado para a leitura dos dados.

---

## üß† Algoritmo de Regress√£o Linear

O projeto implementa o algoritmo de **Gradiente Descendente** (Gradient Descent) para otimizar os par√¢metros do modelo ($\theta_0$ e $\theta_1$), que minimizam a **Fun√ß√£o de Custo** (Mean Squared Error - MSE).

### Modelo Matem√°tico

O modelo de predi√ß√£o √© dado pela fun√ß√£o de hip√≥tese:
$$
h_\theta(x) = \theta_0 + \theta_1 x
$$

Onde:
* $x$ √© a vari√°vel de entrada (e.g., Ciclos de Carga).
* $h_\theta(x)$ √© a predi√ß√£o do SOH.
* $\theta_0$ (intercepto) e $\theta_1$ (coeficiente angular) s√£o os par√¢metros aprendidos.

### Fun√ß√£o de Custo (MSE)

A fun√ß√£o que o algoritmo busca minimizar √© a Custo Quadr√°tico M√©dio (MSE):
$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

### Atualiza√ß√£o de Par√¢metros (Gradiente Descendente)

Os par√¢metros s√£o atualizados iterativamente (onde $\alpha$ √© a taxa de aprendizado):
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})
$$
$$
\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}
$$


